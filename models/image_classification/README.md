# Transformers

### An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
Paper: [https://arxiv.org/pdf/2010.11929](https://arxiv.org/pdf/2010.11929)

### Swin Tranformer:  Hierarchical Vision Transformer using Shifted Windows
Paper: [https://arxiv.org/pdf/2103.14030](https://arxiv.org/pdf/2103.14030)

### Conditional Positional Encoders for Vision Transformers

**Paper:** [Read the Paper](https://arxiv.org/pdf/2102.10882)

**Introduction:**
This paper introduces a novel approach called Conditional Positional Encoding (CPE) to enhance Vision Transformers (ViT) in visual recognition tasks. Traditional solutions like ViT have limitations in adapting to different image sizes during testing, affecting their flexibility. CPE addresses this issue by dynamically generating positional encodings based on nearby tokens in the image. This flexibility enables the transformer to adjust to various image sizes and accurately locate objects, leading to improved performance in tasks like object detection and segmentation.

**Quick Intro:**
CPE, introduced in this paper, offers a dynamic way of adding positional information to Vision Transformers (ViT). By generating encodings based on nearby tokens in an image, CPE enhances the flexibility of transformers during inference, enabling them to adapt to different image sizes and accurately locate objects. This approach, termed Conditional Position encoding Vision Transformer (CPVT), improves the performance of various vision tasks like object detection and segmentation, and works well with different input resolutions.


### Transformer in Transformer (TNT)
**Paper:** [Read the Paper](https://arxiv.org/pdf/2103.00112)
